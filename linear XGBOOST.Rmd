---
title: "xgboost"
author: "Jinran Li"
date: "2025-05-01"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(dplyr)
library(lubridate)
library(tidyr)
library(stringr)
library(tibble)
library(fastDummies)
library(ggplot2)
library(tidyverse)
library(DataExplorer)
library(corrplot)


```

# Exploratory Data Analysis (EDA)
```{r}
EDAdata <- read.csv("raw_modeling_data.csv")

# Plot a histogram showing the distribution of customer outages per event/day
  ggplot(EDAdata, aes(x = customers_day)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Customer Outages (log scale)",
       x = "Customer Outages (log10)", y = "Count") +
  theme_minimal()

# Filter the top 15 states with the highest total outages
if ("state" %in% names(EDAdata)) {
  EDAdata %>%
    group_by(state) %>%
    summarise(total_outages = sum(customers_day, na.rm = TRUE)) %>%
    arrange(desc(total_outages)) %>%
    top_n(15, total_outages) %>%
    ggplot(aes(x = reorder(state, total_outages), y = total_outages)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(title = "Top 15 States by Total Outages",
         x = "State", y = "Total Outages") +
    theme_minimal()
}

# Select the top 10 event types with the highest average
if ("EVENT_TYPE" %in% names(EDAdata)) {
  EDAdata %>%
    group_by(EVENT_TYPE) %>%
    summarise(avg_outages = mean(customers_day, na.rm = TRUE),
              count = n()) %>%
    arrange(desc(avg_outages)) %>%
    top_n(10, avg_outages) %>%
    ggplot(aes(x = reorder(EVENT_TYPE, avg_outages), y = avg_outages)) +
    geom_bar(stat = "identity", fill = "darkorange") +
    coord_flip() +
    labs(title = "Top Event Types by Average Customer Outages",
         x = "Event Type", y = "Average Outages") +
    theme_minimal()
}

```


```{r}

library(dplyr)
library(lubridate)

EDAdata <- EDAdata %>%
  mutate(
    BEGIN_DATETIME = as.POSIXct(paste(date, BEGIN_TIME), format = "%Y-%m-%d %H:%M"),
    END_DATETIME   = as.POSIXct(paste(date, END_TIME), format = "%Y-%m-%d %H:%M"),
    DURATION_MINUTES = as.numeric(difftime(END_DATETIME, BEGIN_DATETIME, units = "mins"))
  )

EDAdata %>%
  group_by(EVENT_TYPE) %>%
  summarise(avg_duration = mean(DURATION_MINUTES, na.rm = TRUE)) %>%
  arrange(desc(avg_duration)) %>%
  ggplot(aes(x = reorder(EVENT_TYPE, avg_duration), y = avg_duration)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Average Duration by Event Type",
    x = "Event Type",
    y = "Average Duration (minutes)"
  ) +
  theme_minimal()



```



# Modeling
# # Further Data Cleaning and Combining

We have only 1368 2014 observations, which is a small proportion of the data, so we remove information about 2014.

consider "MAGNITUDE" column
```{r}
data <- read.csv("raw_modeling_data.csv")
#head(data)

# Modeling data cleaning process
data[data == ""] <- NA

na_ratio <- sapply(data, function(x) mean(is.na(x)))
data <- data[, na_ratio <= 0.4] # NA ratio for modeling data

removed_cols <- names(na_ratio[na_ratio > 0.4])
#print(removed_cols) # Show dropped columns

# Remove useless columns
# List of columns to remove
columns_to_remove <- c(
  "X",                  # Row ID, meaningless
  "CZ_TYPE",            # Area type (low variance, not useful)
  "BEGIN_AZIMUTH",      # Compass direction (messy, text-based)
  "END_AZIMUTH",        # Compass direction (messy, text-based)
  "BEGIN_LOCATION",     # Text description, not structured
  "END_LOCATION",       # Text description
  "UTC_OFFSET"          # Timezone offset (redundant with 'state')
)

# Remove these columns
data <- data[, !(names(data) %in% columns_to_remove)]

# View the remaining columns
#print(names(data))

# Rearrange the time format
data <- data %>%
  mutate(
    year = year(date),
    month = month(date),
    day = day(date),
    weekday = wday(date, week_start = 1) - 1,
    is_weekend = ifelse(weekday %in% c(5, 6), 1, 0)
  ) %>%
  select(year, month, day, weekday, is_weekend, everything(), -date)

data$begin_hour <- hour(parse_date_time(data$BEGIN_TIME, orders = "H:M"))
data$begin_hour[is.na(data$begin_hour)] <- -1

# Count the number of occurrences per year
year_counts <- table(data$year)
print(year_counts)

# Remove rows where the year is 2014
data <- data[data$year != "2014", ]

# Feature Engineering
# Time Management
data$BEGIN_TIME <- hm(data$BEGIN_TIME)
data$END_TIME <- hm(data$END_TIME)

# Calculate raw duration
minute_difference <- as.numeric(as.duration(data$END_TIME - data$BEGIN_TIME)) / 60

# Handle negative time differences
minute_difference[minute_difference < 0] <- 
  minute_difference[minute_difference < 0] + 24 * 60

# Calculate day span difference
day_difference <- data$END_DAY - data$BEGIN_DAY
data$duration_minutes <- day_difference * 24 * 60 + minute_difference

# Remove useless columns after engineering
data <- data[, !(names(data) %in% c("BEGIN_TIME", "END_TIME", "BEGIN_DAY", "END_DAY"))]
head(data)

# Damage Convention
convert_damage <- function(x) {
  x <- toupper(trimws(as.character(x)))  # standardize
  sapply(x, function(val) {
    if (is.na(val) || val == "") {
      return(0)
    } else if (grepl("K$", val)) {
      return(as.numeric(gsub("K", "", val)) * 1e3)
    } else if (grepl("M$", val)) {
      return(as.numeric(gsub("M", "", val)) * 1e6)
    } else if (grepl("B$", val)) {
      return(as.numeric(gsub("B", "", val)) * 1e9)
    } else if (!is.na(as.numeric(val))) {
      return(as.numeric(val))
    } else {
      return(0)
    }
  })
}

data <- data %>%
  mutate(
    damage_property = replace_na(convert_damage(DAMAGE_PROPERTY), 0),
    damage_crops = replace_na(convert_damage(DAMAGE_CROPS), 0)
  )


# One-hot encode EVENT_TYPE and state
final_data <- dummy_cols(data, 
                         select_columns = c("EVENT_TYPE", "state"), 
                         remove_first_dummy = FALSE, 
                         remove_selected_columns = TRUE)

# Remove original EVENT_TYPE, state, and county columns from original data
data_cleaned <- data %>% select(-EVENT_TYPE, -state, -county)

# Re-bind the cleaned original data with the new dummy variables
final_data <- bind_cols(data_cleaned, 
                        select(final_data, starts_with("EVENT_TYPE_"), starts_with("state_")))



# Perform one-hot encoding for the EVENT_TYPE column
# This will generate binary indicator columns like EVENT_TYPE_Tornado, EVENT_TYPE_Hail, etc.
final_data <- dummy_cols(data, select_columns = "EVENT_TYPE", remove_first_dummy = FALSE, remove_selected_columns = TRUE)
data_cleaned <- data %>% select(-EVENT_TYPE)
final_data <- bind_cols(data_cleaned, select(final_data, starts_with("EVENT_TYPE_")))


```

```{r}
# Step 1: dummy 编码整个 final_data
library(fastDummies)
final_data <- dummy_cols(
  final_data,
  select_columns = "state",
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)

```
```{r}
# Step 2: 划分训练集 / 测试集
library(caret)
set.seed(123)
train_idx <- createDataPartition(final_data$customers_day, p = 0.8, list = FALSE)
train_data <- final_data[train_idx, ]
test_data  <- final_data[-train_idx, ]
```

```{r}
library(xgboost)

X <- final_data %>%
  select(-customers_day) %>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()

y <- final_data$customers_day

# split as before
set.seed(123)
train_idx <- sample(1:nrow(X), size = 0.8 * nrow(X))
X_train <- X[train_idx, ]
X_test <- X[-train_idx, ]
y_train <- y[train_idx]
y_test <- y[-train_idx]

# create DMatrix
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)




```


```{r}

params <- list(
  objective = "reg:squarederror",  # 回归任务目标函数
  eta = 0.1,                        # 学习率
  max_depth = 6                    # 树的最大深度
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,                   # 迭代次数
  watchlist = list(train = dtrain, test = dtest),
  print_every_n = 10,
  early_stopping_rounds = 10       # 如果 10 轮都没有提升就停止
)


```

```{r}
y_pred <- predict(xgb_model, newdata = dtest)
plot(y_test, y_pred,
     xlab = "Actual", ylab = "Predicted",
     main = "XGBoost Prediction vs Actual")
abline(0, 1, col = "red")

```
```{r}
mse <- mean((y_test - y_pred)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_test - y_pred))
cat("MSE:", mse, "RMSE:", rmse, "MAE:", mae)

```

```{r}
# 预测值（连续）
y_pred_prob <- predict(xgb_model, newdata = dtest)

# 转为二分类（0/1），阈值可调
y_pred_class <- ifelse(y_pred_prob > 0.5, 1, 0)

```

```{r}
# 安装（如未安装）并加载
# install.packages("pROC")
library(pROC)

# 生成 ROC 对象
roc_obj <- roc(y_test, y_pred_prob)

# 绘制 ROC 曲线
plot(roc_obj, col = "blue", lwd = 2,
     main = paste("ROC Curve (AUC =", round(auc(roc_obj), 3), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")

```

```{r}
table(y_test)
```









