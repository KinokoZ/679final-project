---
title: "MA 679 Final Project"
author: "Zihao Zhang, Keyu Feng, Jinran Li, Ziheng Li"
output: pdf_document
date: "2025-04-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

<<<<<<< HEAD
=======



>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46
library(dplyr)
library(lubridate)
library(tidyr)
library(stringr)
library(tibble)
library(fastDummies)
library(ggplot2)
library(tidyverse)
library(DataExplorer)
library(corrplot)

<<<<<<< HEAD
=======


>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46
```
# Abstract


# Introduction


# Separate Data Cleaning and Modeling Data Combination
# # Outage Data (No need to run again)
```{r}
# Load the data
outage_data <- read.csv("/projectnb/msspbc/keyu/eaglei_outages_2014_2023.csv")
#head(outage_data)

# Rearrange the data
# Step 1: Make sure time column is proper datetime
df <- outage_data %>%
  mutate(run_start_time = ymd_hms(run_start_time),
         date = as.Date(run_start_time))

# Step 2: Group by state + date, sum the customers, and convert to customers.day
summary <- df %>%
  group_by(state, county, date) %>%
  summarise(
    total_customers = sum(customers_out, na.rm = TRUE),
    customers_day = total_customers / 96,
    .groups = "drop"
  )

# Load the new dataset
#write.csv(summary, "outage.csv")

```

# # Stromevents Data (No need to run again)
```{r}
# Access the data
storm_data <- read.csv("StormEvents_2014_2024.csv")
#head(storm_data)

# Timezone offset table 
timezone_offset <- tibble(
  CZ_TIMEZONE = c("CST", "CDT", "EST", "EDT", "MST", "MDT", "PST", "PDT", "HST", "AKST", "AKDT"),
  UTC_OFFSET = c(-6, -5, -5, -4, -7, -6, -8, -7, -10, -9, -8)
)

# Time format cleaning + UTC adjustment 
storm_data <- storm_data %>%
  mutate(
    BEGIN_TIME = str_pad(as.character(BEGIN_TIME), 4, pad = "0"),
    END_TIME = str_pad(as.character(END_TIME), 4, pad = "0"),
    BEGIN_TIME = paste0(str_sub(BEGIN_TIME, 1, 2), ":", str_sub(BEGIN_TIME, 3, 4)),
    END_TIME = paste0(str_sub(END_TIME, 1, 2), ":", str_sub(END_TIME, 3, 4))
  ) %>%
  left_join(timezone_offset, by = "CZ_TIMEZONE") %>%
  mutate(
    UTC_OFFSET = ifelse(is.na(UTC_OFFSET), 0, UTC_OFFSET),
    MONTH_NAME = str_pad(match(MONTH_NAME, month.name), 2, pad = "0"),
    BEGIN_DAY = str_pad(BEGIN_DAY, 2, pad = "0"),
    END_DAY = str_pad(END_DAY, 2, pad = "0"),
    BEGIN_DATE_TIME = ymd_hm(paste0(YEAR, "-", MONTH_NAME, "-", BEGIN_DAY, " ", BEGIN_TIME)) + hours(UTC_OFFSET),
    END_DATE_TIME = ymd_hm(paste0(YEAR, "-", MONTH_NAME, "-", END_DAY, " ", END_TIME)) + hours(UTC_OFFSET),
    DATE = as.Date(BEGIN_DATE_TIME),
    state = str_to_upper(STATE),
    county = str_to_lower(str_trim(CZ_NAME))
  )

# Select and clean final stormevents_filtered for join
stormevents_filtered <- storm_data %>%
  filter(EVENT_TYPE %in% {
    top_events <- count(., EVENT_TYPE, sort = TRUE) %>%
      top_n(20, n) %>%
      pull(EVENT_TYPE)
    union(top_events, c("Hurricane", "Wildfire", "Lightning", "Ice Storm"))
  }) %>%
  mutate(
    date = DATE
  ) %>%
  select(date, state, county, EVENT_TYPE, BEGIN_TIME, END_TIME, everything()) %>%
  arrange(state, date)

# Optional: Check NA summary
calculate_na_summary <- function(data) {
  na_info <- sapply(data, function(x) sum(is.na(x)))
  data.frame(
    Column = names(na_info),
    NA_Count = as.vector(na_info),
    Total_Rows = nrow(data),
    NA_Percentage = round((as.vector(na_info) / nrow(data)) * 100, 2)
  ) %>%
    arrange(desc(NA_Percentage))
}

na_summary <- calculate_na_summary(stormevents_filtered)
print(na_summary)

stormevents_filtered <- stormevents_filtered %>%
  arrange(STATE, DATE) %>% select(-BEGIN_YEARMONTH, -END_YEARMONTH, -EPISODE_ID, -EVENT_ID, -STATE, -STATE_FIPS, -MONTH_NAME, -CZ_FIPS, -CZ_NAME, -BEGIN_DATE_TIME, -CZ_TIMEZONE, -END_DATE_TIME, -SOURCE, -CATEGORY, -TOR_OTHER_CZ_FIPS, -TOR_LENGTH, -TOR_WIDTH, -TOR_OTHER_CZ_NAME,  -EPISODE_NARRATIVE, -EVENT_NARRATIVE, -DATA_SOURCE, -DATE, -YEAR)
#head(stormevents_filtered)

```

There are 3 columns that contains > 50% NAs, we removed them.

# # Combine the datasets by matching state and date. (No need to run again)
```{r}
# Loading cleaning outage set for convenience
outage_data <- read.csv("outage.csv")
#head(outage_data)

# Modify the outage dataset: standardize fields
outage_data <- outage_data %>%
  mutate(
    state = str_to_upper(state),
    county = str_to_lower(county),
    date = as.Date(date)
  )
#head(outage_data)

# Modify the stormevents_filtered dataset: ensure matchable format
stormevents_filtered <- stormevents_filtered %>%
  mutate(
    state = str_to_upper(state),
    county = str_to_lower(county),
    date = as.Date(date)
  )
#head(stormevents_filtered)

# Combine datasets using state + county + date as key
modeling_data <- inner_join(
  outage_data,
  stormevents_filtered,
  by = c("state", "county", "date")
) %>%
  arrange(state, date)

# Write the combination dataset (optional)
#write.csv(modeling_data, "raw_modeling_data.csv", row.names = FALSE)

```

<<<<<<< HEAD

=======
```{r}
colnames(EDAdata)

```
>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46

# Exploratory Data Analysis (EDA)
```{r}
EDAdata <- read.csv("raw_modeling_data.csv")

# Plot a histogram showing the distribution of customer outages per event/day
  ggplot(EDAdata, aes(x = customers_day)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Customer Outages (log scale)",
       x = "Customer Outages (log10)", y = "Count") +
  theme_minimal()

# Filter the top 15 states with the highest total outages
if ("state" %in% names(EDAdata)) {
  EDAdata %>%
    group_by(state) %>%
    summarise(total_outages = sum(customers_day, na.rm = TRUE)) %>%
    arrange(desc(total_outages)) %>%
    top_n(15, total_outages) %>%
    ggplot(aes(x = reorder(state, total_outages), y = total_outages)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(title = "Top 15 States by Total Outages",
         x = "State", y = "Total Outages") +
    theme_minimal()
}

# Select the top 10 event types with the highest average
if ("EVENT_TYPE" %in% names(EDAdata)) {
  EDAdata %>%
    group_by(EVENT_TYPE) %>%
    summarise(avg_outages = mean(customers_day, na.rm = TRUE),
              count = n()) %>%
    arrange(desc(avg_outages)) %>%
    top_n(10, avg_outages) %>%
    ggplot(aes(x = reorder(EVENT_TYPE, avg_outages), y = avg_outages)) +
    geom_bar(stat = "identity", fill = "darkorange") +
    coord_flip() +
    labs(title = "Top Event Types by Average Customer Outages",
         x = "Event Type", y = "Average Outages") +
    theme_minimal()
}

```
<<<<<<< HEAD
=======
```{r}

library(dplyr)
library(lubridate)

EDAdata <- EDAdata %>%
  mutate(
    BEGIN_DATETIME = as.POSIXct(paste(date, BEGIN_TIME), format = "%Y-%m-%d %H:%M"),
    END_DATETIME   = as.POSIXct(paste(date, END_TIME), format = "%Y-%m-%d %H:%M"),
    DURATION_MINUTES = as.numeric(difftime(END_DATETIME, BEGIN_DATETIME, units = "mins"))
  )

EDAdata %>%
  group_by(EVENT_TYPE) %>%
  summarise(avg_duration = mean(DURATION_MINUTES, na.rm = TRUE)) %>%
  arrange(desc(avg_duration)) %>%
  ggplot(aes(x = reorder(EVENT_TYPE, avg_duration), y = avg_duration)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Average Duration by Event Type",
    x = "Event Type",
    y = "Average Duration (minutes)"
  ) +
  theme_minimal()



```


```{r}
library(ggplot2)
EDAdata %>%
  group_by(YEAR, EVENT_TYPE) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = YEAR, y = count, color = EVENT_TYPE)) +
  geom_line() +
  labs(title = "Event Type Frequency by Year")

```
```{r}
EDAdata %>%
  group_by(state, EVENT_TYPE) %>%
  summarise(count = n()) %>%
  filter(EVENT_TYPE == "Tornado") %>%  # Example of looking at Tornado only, with the possibility of switching to other events
  ggplot(aes(x = reorder(state, count), y = count)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Tornado Frequency by State")

```




>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46

# Modeling
# # Further Data Cleaning and Combining

We have only 1368 2014 observations, which is a small proportion of the data, so we remove information about 2014.

consider "MAGNITUDE" column
```{r}
data <- read.csv("raw_modeling_data.csv")
#head(data)

# Modeling data cleaning process
data[data == ""] <- NA

na_ratio <- sapply(data, function(x) mean(is.na(x)))
data <- data[, na_ratio <= 0.4] # NA ratio for modeling data

removed_cols <- names(na_ratio[na_ratio > 0.4])
#print(removed_cols) # Show dropped columns

# Remove useless columns
# List of columns to remove
columns_to_remove <- c(
  "X",                  # Row ID, meaningless
  "CZ_TYPE",            # Area type (low variance, not useful)
  "BEGIN_AZIMUTH",      # Compass direction (messy, text-based)
  "END_AZIMUTH",        # Compass direction (messy, text-based)
  "BEGIN_LOCATION",     # Text description, not structured
  "END_LOCATION",       # Text description
  "UTC_OFFSET"          # Timezone offset (redundant with 'state')
)

# Remove these columns
data <- data[, !(names(data) %in% columns_to_remove)]

# View the remaining columns
#print(names(data))

# Rearrange the time format
data <- data %>%
  mutate(
    year = year(date),
    month = month(date),
    day = day(date),
    weekday = wday(date, week_start = 1) - 1,
    is_weekend = ifelse(weekday %in% c(5, 6), 1, 0)
  ) %>%
  select(year, month, day, weekday, is_weekend, everything(), -date)

data$begin_hour <- hour(parse_date_time(data$BEGIN_TIME, orders = "H:M"))
data$begin_hour[is.na(data$begin_hour)] <- -1

# Count the number of occurrences per year
year_counts <- table(data$year)
print(year_counts)

# Remove rows where the year is 2014
data <- data[data$year != "2014", ]

# Feature Engineering
# Time Management
data$BEGIN_TIME <- hm(data$BEGIN_TIME)
data$END_TIME <- hm(data$END_TIME)

# Calculate raw duration
minute_difference <- as.numeric(as.duration(data$END_TIME - data$BEGIN_TIME)) / 60

# Handle negative time differences
minute_difference[minute_difference < 0] <- 
  minute_difference[minute_difference < 0] + 24 * 60

# Calculate day span difference
day_difference <- data$END_DAY - data$BEGIN_DAY
data$duration_minutes <- day_difference * 24 * 60 + minute_difference

# Remove useless columns after engineering
data <- data[, !(names(data) %in% c("BEGIN_TIME", "END_TIME", "BEGIN_DAY", "END_DAY"))]
head(data)

# Damage Convention
convert_damage <- function(x) {
  x <- toupper(trimws(as.character(x)))  # standardize
  sapply(x, function(val) {
    if (is.na(val) || val == "") {
      return(0)
    } else if (grepl("K$", val)) {
      return(as.numeric(gsub("K", "", val)) * 1e3)
    } else if (grepl("M$", val)) {
      return(as.numeric(gsub("M", "", val)) * 1e6)
    } else if (grepl("B$", val)) {
      return(as.numeric(gsub("B", "", val)) * 1e9)
    } else if (!is.na(as.numeric(val))) {
      return(as.numeric(val))
    } else {
      return(0)
    }
  })
}

data <- data %>%
  mutate(
    damage_property = replace_na(convert_damage(DAMAGE_PROPERTY), 0),
    damage_crops = replace_na(convert_damage(DAMAGE_CROPS), 0)
  )

<<<<<<< HEAD
# One-hot encode EVENT_TYPE and state
final_data <- dummy_cols(data, 
                         select_columns = c("EVENT_TYPE", "state"), 
                         remove_first_dummy = FALSE, 
                         remove_selected_columns = TRUE)

# Remove original EVENT_TYPE, state, and county columns from original data
data_cleaned <- data %>% select(-EVENT_TYPE, -state, -county)

# Re-bind the cleaned original data with the new dummy variables
final_data <- bind_cols(data_cleaned, 
                        select(final_data, starts_with("EVENT_TYPE_"), starts_with("state_")))


=======
# Perform one-hot encoding for the EVENT_TYPE column
# This will generate binary indicator columns like EVENT_TYPE_Tornado, EVENT_TYPE_Hail, etc.
final_data <- dummy_cols(data, select_columns = "EVENT_TYPE", remove_first_dummy = FALSE, remove_selected_columns = TRUE)
data_cleaned <- data %>% select(-EVENT_TYPE)
final_data <- bind_cols(data_cleaned, select(final_data, starts_with("EVENT_TYPE_")))
>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46

```


<<<<<<< HEAD
```{r}
library(glmnet)
library(tidyverse)
# Step 1: 准备特征矩阵 X 和目标变量 y
X <- final_data %>%
  select(-customers_day) %>%
  as.matrix()

y <- final_data$customers_day

# Step 2: 拆分训练集和测试集（可选）
set.seed(123)
train_idx <- sample(1:nrow(X), size = 0.8 * nrow(X))
X_train <- X[train_idx, ]
X_test  <- X[-train_idx, ]
y_train <- y[train_idx]
y_test  <- y[-train_idx]
X_train[is.na(X_train)] <- 0
X_test[is.na(X_test)] <- 0

# Step 3: 拟合 Lasso 模型（alpha = 1 表示 Lasso）
lasso_model <- glmnet(X_train, y_train, alpha = 1)

# Step 4: 使用交叉验证选择最佳 lambda
cv_model <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_model$lambda.min
cat("Best lambda:", best_lambda, "\n")

# Step 5: 在测试集上预测并计算 MSE
y_pred <- predict(cv_model, s = best_lambda, newx = X_test)
mse <- mean((y_pred - y_test)^2)
cat("Test MSE:", mse, "\n")




=======

```{r}


library(dplyr)
library(ggplot2)

event_vars <- names(final_data)[grepl("^EVENT_TYPE_", names(final_data))]


avg_outage_per_event <- lapply(event_vars, function(col) {
  final_data %>%
    filter(.data[[col]] == 1) %>%
    summarise(event_type = col,
              avg_outage = mean(customers_day, na.rm = TRUE))
}) %>%
  bind_rows() %>%
  arrange(desc(avg_outage))


ggplot(avg_outage_per_event, aes(x = reorder(event_type, avg_outage), y = avg_outage)) +
  geom_col(fill = "orange") +
  coord_flip() +
  labs(title = "Average Customer Outages by Event Type",
       x = "Event Type", y = "Average Outage") +
  theme_minimal()


```
```{r}
total_outage_per_event <- lapply(event_vars, function(col) {
  final_data %>%
    filter(.data[[col]] == 1) %>%
    summarise(event_type = col,
              total_outage = sum(customers_day, na.rm = TRUE))
}) %>%
  bind_rows() %>%
  arrange(desc(total_outage))

ggplot(total_outage_per_event, aes(x = reorder(event_type, total_outage), y = total_outage)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Total Customer Outages by Event Type",
       x = "Event Type", y = "Total Outage") +
  theme_minimal()
>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46

```


<<<<<<< HEAD
=======


>>>>>>> c6a16b79d9957b020c2538365e25117063dc4a46
