---
title: "xgboost"
author: "Jinran Li"
date: "2025-05-01"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(dplyr)
library(lubridate)
library(tidyr)
library(stringr)
library(tibble)
library(fastDummies)
library(ggplot2)
library(tidyverse)
library(DataExplorer)
library(corrplot)


```





# Modeling
# # Further Data Cleaning and Combining

We have only 1368 2014 observations, which is a small proportion of the data, so we remove information about 2014.

consider "MAGNITUDE" column

```{r}
data <- read.csv("raw_modeling_data.csv")
#head(data)

# Modeling data cleaning process
data[data == ""] <- NA

na_ratio <- sapply(data, function(x) mean(is.na(x)))
data <- data[, na_ratio <= 0.4] # NA ratio for modeling data

removed_cols <- names(na_ratio[na_ratio > 0.4])
#print(removed_cols) # Show dropped columns

# Remove useless columns
# List of columns to remove
columns_to_remove <- c(
  "X",                  # Row ID, meaningless
  "CZ_TYPE",            # Area type (low variance, not useful)
  "BEGIN_AZIMUTH",      # Compass direction (messy, text-based)
  "END_AZIMUTH",        # Compass direction (messy, text-based)
  "BEGIN_LOCATION",     # Text description, not structured
  "END_LOCATION",       # Text description
  "UTC_OFFSET"          # Timezone offset (redundant with 'state')
)

# Remove these columns
data <- data[, !(names(data) %in% columns_to_remove)]

# View the remaining columns
#print(names(data))

# Rearrange the time format
data <- data %>%
  mutate(
    year = year(date),
    month = month(date),
    day = day(date),
    weekday = wday(date, week_start = 1) - 1,
    is_weekend = ifelse(weekday %in% c(5, 6), 1, 0)
  ) %>%
  select(year, month, day, weekday, is_weekend, everything(), -date)

data$begin_hour <- hour(parse_date_time(data$BEGIN_TIME, orders = "H:M"))
data$begin_hour[is.na(data$begin_hour)] <- -1

# Count the number of occurrences per year
year_counts <- table(data$year)
print(year_counts)

# Remove rows where the year is 2014
data <- data[data$year != "2014", ]

# Feature Engineering
# Time Management
data$BEGIN_TIME <- hm(data$BEGIN_TIME)
data$END_TIME <- hm(data$END_TIME)

# Calculate raw duration
minute_difference <- as.numeric(as.duration(data$END_TIME - data$BEGIN_TIME)) / 60

# Handle negative time differences
minute_difference[minute_difference < 0] <- 
  minute_difference[minute_difference < 0] + 24 * 60

# Calculate day span difference
day_difference <- data$END_DAY - data$BEGIN_DAY
data$duration_minutes <- day_difference * 24 * 60 + minute_difference

# Remove useless columns after engineering
data <- data[, !(names(data) %in% c("BEGIN_TIME", "END_TIME", "BEGIN_DAY", "END_DAY"))]
head(data)

# Damage Convention
convert_damage <- function(x) {
  x <- toupper(trimws(as.character(x)))  # standardize
  sapply(x, function(val) {
    if (is.na(val) || val == "") {
      return(0)
    } else if (grepl("K$", val)) {
      return(as.numeric(gsub("K", "", val)) * 1e3)
    } else if (grepl("M$", val)) {
      return(as.numeric(gsub("M", "", val)) * 1e6)
    } else if (grepl("B$", val)) {
      return(as.numeric(gsub("B", "", val)) * 1e9)
    } else if (!is.na(as.numeric(val))) {
      return(as.numeric(val))
    } else {
      return(0)
    }
  })
}

data <- data %>%
  mutate(
    damage_property = replace_na(convert_damage(DAMAGE_PROPERTY), 0),
    damage_crops = replace_na(convert_damage(DAMAGE_CROPS), 0)
  )


# One-hot encode EVENT_TYPE and state
final_data <- dummy_cols(data, 
                         select_columns = c("EVENT_TYPE", "state"), 
                         remove_first_dummy = FALSE, 
                         remove_selected_columns = TRUE)

# Remove original EVENT_TYPE, state, and county columns from original data
data_cleaned <- data %>% select(-EVENT_TYPE, -state, -county)

# Re-bind the cleaned original data with the new dummy variables
final_data <- bind_cols(data_cleaned, 
                        select(final_data, starts_with("EVENT_TYPE_"), starts_with("state_")))



# Perform one-hot encoding for the EVENT_TYPE column
# This will generate binary indicator columns like EVENT_TYPE_Tornado, EVENT_TYPE_Hail, etc.
final_data <- dummy_cols(data, select_columns = "EVENT_TYPE", remove_first_dummy = FALSE, remove_selected_columns = TRUE)
data_cleaned <- data %>% select(-EVENT_TYPE)
final_data <- bind_cols(data_cleaned, select(final_data, starts_with("EVENT_TYPE_")))


```

# XGBoost is an integrated learning method that effectively reduces bias and controls overfitting by boosting iterations of multiple weak learners (usually regression trees), often achieving leading performance in modeling structured data
```{r}
  # Step 1: dummy encodes the entire final_data
library(fastDummies)
final_data <- dummy_cols(
  final_data,
  select_columns = "state",
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)

```

```{r}

# Step 2: Divide training/test set
library(caret)
set.seed(123)
train_idx <- createDataPartition(final_data$customers_day, p = 0.8, list = FALSE)
train_data <- final_data[train_idx, ]
test_data  <- final_data[-train_idx, ]
```

```{r}
library(xgboost)

X <- final_data %>%
  select(-customers_day) %>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()

y <- final_data$customers_day

# split as before
set.seed(123)
train_idx <- sample(1:nrow(X), size = 0.8 * nrow(X))
X_train <- X[train_idx, ]
X_test <- X[-train_idx, ]
y_train <- y[train_idx]
y_test <- y[-train_idx]

# create DMatrix
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)




```

# XGBoost
```{r}

params <- list(
  objective = "reg:squarederror",   # Regression task objective function
  eta = 0.1,                        # learning rate
  max_depth = 6                    # Maximum depth of the tree
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,                   # Number of iterations
  watchlist = list(train = dtrain, test = dtest),
  print_every_n = 10,
  early_stopping_rounds = 10       # If don't improve in 10 rounds, stop.
)


```
The model quickly reduces the error on the training set and finally converges to RMSE = 35.91 on the test set, indicating that the model successfully captures the important patterns in the data. Despite some overfitting, the model shows good generalization ability on the test set due to the use of the early stopping mechanism



#Plot the result
```{r}
y_pred <- predict(xgb_model, newdata = dtest)
plot(y_test, y_pred,
     xlab = "Actual", ylab = "Predicted",
     main = "XGBoost Prediction vs Actual")
abline(0, 1, col = "red")

```
Scatterplot showing the relationship between the predicted and true customers_day values of the XGBoost model on the test set.

Most of the data points are close to the diagonal line, indicating that the predicted values of the model are generally close to the actual values and have a good fit.


The XGBoost model has strong predictive power on the test set, with most of the predicted values in good agreement with the actual values, and only a few extreme cases have errors in prediction.



# Check MSE,RMSE and MAE
```{r}
mse <- mean((y_test - y_pred)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_test - y_pred))
cat("MSE:", mse, "RMSE:", rmse, "MAE:", mae)

```

The RMSE and MAE are both relatively small, indicating that the overall prediction bias of the model is small.

The RMSE is significantly larger than the MAE, indicating that there may be a small number of large error values in the data (e.g., extreme outage events) that may have some effect on the model.

Overall, the model has high fitting accuracy and is suitable for predicting customers_day.















